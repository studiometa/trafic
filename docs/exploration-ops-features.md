# Exploration: Ops & Management Features

> Status: **exploration** — evaluating approaches, not yet committed to implementation.

## Table of Contents

1. [Backup Solution](#1-backup-solution)
2. [Fleet Management from a Dev Machine](#2-fleet-management-from-a-dev-machine)
3. [Server Hardening](#3-server-hardening)
4. [HTTPS](#4-https)
5. [Per-Project Config](#5-per-project-config)

---

## 1. Backup Solution

### The Problem

A DDEV preview server hosts dozens of projects, each with:
- Source code (`~/www/<project>/`)
- DDEV containers (MySQL/MariaDB databases, uploaded files)
- Agent state (SQLite DB at `/var/lib/trafic/db.sqlite`)
- Agent config (`/etc/trafic/config.toml`)

If the server dies, everything is gone. Source code is recoverable from Git, but **databases and uploaded files are not**.

### What Needs to Be Backed Up

| Data | Location | Priority | Size | Recovery |
|------|----------|----------|------|----------|
| Agent config | `/etc/trafic/config.toml` | Critical | <1 KB | Must restore before agent can start |
| Agent DB | `/var/lib/trafic/db.sqlite` | Medium | <10 MB | Auth tokens, deploy logs. Recreatable but annoying |
| Project databases | Inside Docker volumes | High | 10 MB–2 GB each | `ddev export-db` per project |
| Uploaded files | `~/www/<project>/web/uploads/` (varies) | Medium | Variable | Often synced from production or CI |
| Source code | `~/www/<project>/` | Low | Recoverable from Git |
| Traefik config | `/etc/traefik/` | Medium | <10 KB | Can be regenerated by `trafic setup` |
| Let's Encrypt certs | Traefik's acme.json | Low | Auto-renewed | ACME regenerates them automatically |

### Approach A: Agent-Level Scheduled Backup (Recommended)

Add a scheduled task to the trafic agent that exports project databases periodically.

```
tasks/
└── backup.ts       # New scheduled task
```

**How it works:**

1. Agent reads `project_list.yaml` to get all DDEV projects
2. For each project with status `running` or `stopped` (start it if stopped):
   - `ddev export-db -f /tmp/backup-<name>.sql.gz`
   - Move to backup directory
3. Backup the agent DB and config
4. Optionally push to a remote (S3, rsync to another server, rclone)

**Config:**

```toml
[backup]
enabled = true
schedule = "0 3 * * *"         # Daily at 3am
retain_days = 7                # Keep 7 days of backups
local_dir = "/var/backups/trafic"

# Optional: push to remote storage
[backup.remote]
type = "s3"                    # "s3" | "rsync" | "rclone"
bucket = "my-trafic-backups"
prefix = "server-name/"
# For rsync:
# type = "rsync"
# target = "backup@storage.example.com:/backups/trafic/"
```

**Backup script per project (what `ddev export-db` does under the hood):**

```bash
# Export database
cd ~/www/<project>
ddev export-db --gzip --file=/var/backups/trafic/<date>/<project>.sql.gz

# Optionally backup uploads (if configured per-project)
tar -czf /var/backups/trafic/<date>/<project>-uploads.tar.gz \
  ~/www/<project>/web/wp-content/uploads/
```

**Restore workflow:**

```bash
# Restore a project from backup
trafic restore --name my-app --date 2026-02-07

# Under the hood:
# 1. ddev start (if not running)
# 2. ddev import-db --file=/var/backups/trafic/2026-02-07/my-app.sql.gz
# 3. (optionally) restore uploads tarball
```

### Approach B: Filesystem Snapshots

If the VPS provider supports snapshots (Hetzner, OVH, DO, etc.), use provider-level snapshots. This is the simplest but:
- Not portable across providers
- Snapshots are expensive storage
- Granularity is the entire disk, not per-project

**Verdict**: Use as a complement, not the primary strategy.

### Approach C: Docker Volume Backup

Backup Docker volumes directly instead of using `ddev export-db`.

```bash
# Stop the container first for consistency
docker stop ddev-<project>-db
docker run --rm \
  -v ddev-<project>_mariadb:/data \
  -v /var/backups:/backup \
  alpine tar czf /backup/<project>-db-volume.tar.gz /data
docker start ddev-<project>-db
```

**Pros**: Captures exact state. **Cons**: Requires stopping the DB container, larger files, not human-readable.

### Recommendation

**Approach A** — agent-level `ddev export-db` per project, with optional remote push.

- Add `trafic backup` CLI command for manual runs
- Add `tasks/backup.ts` to the agent for scheduled runs
- Add `trafic restore` CLI command
- Start with local backups, add remote (S3/rsync) as a config option

### CLI Surface

```bash
# Manual backup of all projects
trafic backup

# Backup a specific project
trafic backup --name my-app

# List available backups
trafic backup --list

# Restore a project
trafic restore --name my-app --date 2026-02-07

# Restore with a specific backup file
trafic restore --name my-app --file /var/backups/trafic/2026-02-07/my-app.sql.gz
```

---

## 2. Fleet Management from a Dev Machine

### The Problem

Studio Meta manages multiple DDEV servers (OVH, Hetzner, client-specific). Today there's no unified way to:
- See all servers and their projects
- Check server health (disk, RAM, uptime)
- Deploy to multiple servers
- Run maintenance across the fleet

### Concept: `trafic fleet`

A CLI subcommand that runs from a developer's machine and talks to multiple servers over SSH.

**Fleet config** (lives on the dev machine, not the server):

```toml
# ~/.config/trafic/fleet.toml

[[servers]]
name = "ovh-preprod"
host = "preprod.ovh.studiometa.fr"
user = "ddev"
tld = "ovh.previews.studiometa.fr"

[[servers]]
name = "hetzner-01"
host = "hetzner01.studiometa.fr"
user = "ddev"
tld = "hz1.previews.studiometa.fr"

[[servers]]
name = "unistra"
host = "192.168.0.47"
user = "ddev"
ssh_options = "-J studiometa@185.155.93.167"
tld = "previews.unistra.fr"
```

### CLI Surface

```bash
# List all servers and their status
trafic fleet status
# ┌──────────────┬──────────────────────────┬──────────┬─────────┬──────────┬──────────┐
# │ Server       │ Host                     │ Projects │ Running │ Disk     │ RAM      │
# ├──────────────┼──────────────────────────┼──────────┼─────────┼──────────┼──────────┤
# │ ovh-preprod  │ preprod.ovh.studiometa.fr│ 24       │ 8       │ 67%      │ 4.2/8 GB │
# │ hetzner-01   │ hetzner01.studiometa.fr  │ 12       │ 3       │ 43%      │ 2.1/4 GB │
# │ unistra      │ 192.168.0.47             │ 6        │ 2       │ 51%      │ 1.8/4 GB │
# └──────────────┴──────────────────────────┴──────────┴─────────┴──────────┴──────────┘

# List all projects across all servers
trafic fleet projects
# ┌──────────────┬─────────────────────┬─────────┬───────────────────────────┐
# │ Server       │ Project             │ Status  │ URL                       │
# ├──────────────┼─────────────────────┼─────────┼───────────────────────────┤
# │ ovh-preprod  │ wordpress-starter   │ running │ wordpress-starter.ovh...  │
# │ ovh-preprod  │ preview-42--starter │ stopped │ preview-42--starter.ovh...│
# │ hetzner-01   │ laravel-app         │ running │ laravel-app.hz1...        │
# └──────────────┴─────────────────────┴─────────┴───────────────────────────┘

# Show details for a specific server
trafic fleet status ovh-preprod

# Run a command on all servers
trafic fleet exec "ddev poweroff"
trafic fleet exec --server ovh-preprod "docker system prune -f"

# Trigger backups on all servers
trafic fleet backup

# Update the trafic agent on all servers
trafic fleet upgrade
```

### Implementation

Under the hood, `trafic fleet` just loops over servers and runs SSH commands. It reuses the existing `ssh.ts` module.

```typescript
// Pseudocode
async function fleetStatus(fleet: FleetConfig): Promise<void> {
  const results = await Promise.allSettled(
    fleet.servers.map(async (server) => {
      const ssh = { ...server, port: server.port ?? 22, sshOptions: server.ssh_options ?? "" };

      const projects = await exec(ssh, "ddev list -j 2>/dev/null | jq length");
      const running = await exec(ssh, "ddev list -j 2>/dev/null | jq '[.[] | select(.status==\"running\")] | length'");
      const disk = await exec(ssh, "df -h / | awk 'NR==2{print $5}'");
      const ram = await exec(ssh, "free -h | awk '/Mem:/{print $3\"/\"$2}'");

      return { name: server.name, projects, running, disk, ram };
    }),
  );
  // ... format table
}
```

### Data Source: Agent API vs SSH

Two approaches for fetching server data:

| Method | Pros | Cons |
|--------|------|------|
| **SSH commands** | Works today, no agent changes needed | Slow (sequential SSH connections), parsing stdout |
| **Agent HTTP API** | Fast (JSON), structured data, richer info | Requires agent to expose `/__fleet__/status` endpoint, needs auth |

**Phase 1**: SSH commands (works now, no agent changes).
**Phase 2**: Agent exposes a `/__status__/fleet` endpoint returning JSON, fleet command uses `curl` over SSH to query it. Much faster and richer.

### Where Does This Live?

This is a **dev machine tool**, not a CI tool. Two options:

1. **Same `@studiometa/trafic-cli` package** — add `trafic fleet` subcommand. Simple, one package.
2. **Separate package** — `@studiometa/trafic-fleet`. Avoids bloating the CI package.

**Recommendation**: Same package. The fleet code is small (just SSH loops + table formatting) and shares the SSH module. The `fleet.toml` config is only read when `trafic fleet` is invoked.

---

## 3. Server Hardening

### The Problem

`trafic setup` creates a working server, but the default setup may lack security best practices. A DDEV server exposed to the internet should be hardened.

### What `trafic setup` Should Do (Hardening Checklist)

#### SSH Hardening

```bash
# /etc/ssh/sshd_config.d/trafic.conf
PermitRootLogin no
PasswordAuthentication no
PubkeyAuthentication yes
MaxAuthTries 3
ClientAliveInterval 300
ClientAliveCountMax 2
AllowUsers ddev deploy
X11Forwarding no
```

- Disable root login
- Disable password auth (keys only)
- Limit allowed users to `ddev` and any deploy users
- Set idle timeout

#### Firewall (UFW)

```bash
# Only allow SSH, HTTP, HTTPS
ufw default deny incoming
ufw default allow outgoing
ufw allow 22/tcp    # SSH
ufw allow 80/tcp    # HTTP (redirect to HTTPS)
ufw allow 443/tcp   # HTTPS
ufw enable
```

Optionally allow monitoring ports (if Grafana agent is installed).

#### Automatic Security Updates

```bash
# Install unattended-upgrades
apt install -y unattended-upgrades
dpkg-reconfigure -plow unattended-upgrades

# /etc/apt/apt.conf.d/50unattended-upgrades
Unattended-Upgrade::Allowed-Origins {
    "${distro_id}:${distro_codename}-security";
};
Unattended-Upgrade::Automatic-Reboot "false";
```

#### Fail2ban

```bash
apt install -y fail2ban

# /etc/fail2ban/jail.local
[sshd]
enabled = true
maxretry = 5
bantime = 3600
findtime = 600
```

Protects against SSH brute force.

#### Docker Security

- Docker daemon only listens on Unix socket (no TCP)
- DDEV containers don't expose ports directly (Traefik handles routing)
- Regular `docker system prune` via cron/agent task

#### System Limits

```bash
# /etc/security/limits.d/trafic.conf
ddev soft nofile 65536
ddev hard nofile 65536
```

#### File Permissions

```bash
# Projects directory
chmod 750 /home/ddev/www
chown ddev:ddev /home/ddev/www

# Config
chmod 600 /etc/trafic/config.toml
chown root:ddev /etc/trafic/config.toml

# Backups
chmod 700 /var/backups/trafic
```

### Implementation in `trafic setup`

Add a `--hardening` flag (or make it default with `--no-hardening` to skip):

```bash
trafic setup --tld=previews.example.com
# Includes hardening by default

trafic setup --tld=previews.example.com --no-hardening
# Skip hardening (for local/dev setups)
```

The setup steps would be:

```
[1] Create ddev user
[2] Install Docker
[3] Install DDEV
[4] Install Node.js (for the agent)
[5] Configure Traefik
[6] Install trafic agent
[7] Harden server          ← new
    ├── SSH config
    ├── UFW firewall
    ├── Unattended upgrades
    ├── Fail2ban
    └── File permissions
```

### Audit Command

Add a `trafic audit` command that checks the server's security posture:

```bash
trafic audit
# ✓ SSH: root login disabled
# ✓ SSH: password auth disabled
# ✓ Firewall: UFW active, ports 22/80/443 only
# ✓ Updates: unattended-upgrades enabled
# ✓ Fail2ban: active
# ⚠ Docker: 3 dangling images (run docker system prune)
# ✗ Disk: 89% used — consider cleanup
```

---

## 4. HTTPS

### Current State

DDEV + Traefik already handles HTTPS through DDEV's built-in Traefik router. But the setup varies:

1. **Local dev**: DDEV uses mkcert for self-signed local certs
2. **Production/preview server**: Needs real TLS certs (Let's Encrypt)

### How DDEV Handles HTTPS on a Server

When DDEV runs on a server with `router_http_port: 80` and `router_https_port: 443`, the DDEV Traefik router can use Let's Encrypt via ACME.

**DDEV global config** (`~/.ddev/global_config.yaml`):

```yaml
router_http_port: "80"
router_https_port: "443"
use_letsencrypt: true
letsencrypt_email: "admin@example.com"
```

This tells DDEV's Traefik to automatically provision Let's Encrypt certs for each project hostname.

### What `trafic setup` Should Configure

```bash
# During setup, configure DDEV global for real HTTPS
ddev config global \
  --router-http-port=80 \
  --router-https-port=443 \
  --use-letsencrypt \
  --letsencrypt-email=admin@example.com
```

### Setup Config

Add HTTPS options to the setup command:

```bash
trafic setup \
  --tld=previews.example.com \
  --letsencrypt-email=admin@example.com
```

Or in the agent config:

```toml
tld = "previews.example.com"

[https]
letsencrypt = true
email = "admin@example.com"
```

### Edge Cases

#### Wildcard Certificates

If using a wildcard TLD like `*.previews.example.com`, Let's Encrypt requires DNS-01 challenge (not HTTP-01). This requires a DNS provider API.

DDEV's Traefik supports this via environment variables:

```yaml
# .ddev/global_config.yaml or via Traefik static config
# For Cloudflare:
# CF_DNS_API_TOKEN=xxx

# For OVH:
# OVH_ENDPOINT=xxx
# OVH_APPLICATION_KEY=xxx
# etc.
```

**Recommendation**: For simplicity, use individual certs per project (HTTP-01 challenge). Wildcard is a nice-to-have for advanced users.

#### Custom Domains

Some projects may need a custom domain (not under the TLD). DDEV handles this via `additional_hostnames` or `additional_fqdns` in `.ddev/config.yaml`. Traefik will auto-provision a cert for it.

```yaml
# .ddev/config.yaml
additional_fqdns:
  - staging.client-project.com
```

### Implementation

Mostly configuration — `trafic setup` just needs to run the right `ddev config global` commands. The heavy lifting is done by DDEV + Traefik.

The agent doesn't need to do anything special for HTTPS. Traefik handles TLS termination and forwards plain HTTP to the agent's forward auth endpoint.

```
Client → HTTPS → Traefik (TLS termination)
                    ├── forwardAuth → Agent (HTTP, localhost)
                    └── route → DDEV container (HTTP, internal Docker network)
```

---

## 5. Per-Project Config

### The Problem

Currently, all deploy behavior is controlled by CLI flags and CI variables. There's no way for a project to declare its own trafic settings. This leads to:

- Repetitive CI config across projects
- No way to set DDEV-specific options per project
- Deploy hooks are limited to `--script`, `--before-script`, `--after-script`

### Proposal: `.trafic.toml` Per-Project Config

A file at the root of each project that declares its trafic deployment settings.

```toml
# .trafic.toml — project-level trafic configuration

[deploy]
# Script to run inside the DDEV container after code sync
script = "composer install --no-dev --no-interaction"

# Paths to rsync from CI artifacts to the server
sync = [
  "web/wp-content/themes/starter/dist/",
  "vendor/",
]

# Hooks
before_script = ""
after_script = ""

# Skip ddev start (e.g. for static projects)
no_start = false

[ddev]
# Override DDEV config.local.yaml values on first deploy
# These are written to .ddev/config.local.yaml when the project is created
php_version = "8.3"
database = { type = "mariadb", version = "10.11" }
webserver_type = "nginx-fpm"
additional_hostnames = []
additional_fqdns = []

# Custom DDEV hooks
[ddev.hooks]
post_start = [
  "composer install --no-dev",
]

[backup]
# Directories to include in project backups (in addition to the database)
include = [
  "web/wp-content/uploads/",
]

[auth]
# Per-project auth override (overrides the global agent config)
# "basic" | "token" | "allow" | "deny"
policy = "basic"
```

### How the CLI Uses It

When `trafic deploy` runs, it:

1. Checks for `.trafic.toml` in the current directory (the CI workspace)
2. Merges file values with CLI flags (CLI flags take precedence)
3. After cloning/pulling on the server, also reads the `.trafic.toml` from the repo for DDEV config

```
CLI flags > .trafic.toml > defaults
```

**Priority chain:**

```typescript
const options = {
  ...defaults,                     // Built-in defaults
  ...projectConfig,                // From .trafic.toml
  ...cliFlags,                     // From command line (highest priority)
};
```

### Examples

#### WordPress Project

```toml
# .trafic.toml
[deploy]
script = "composer install --no-dev --no-interaction --prefer-dist"
sync = ["web/wp-content/themes/starter/dist/"]

[ddev]
php_version = "8.3"
database = { type = "mariadb", version = "10.11" }

[backup]
include = ["web/wp-content/uploads/"]
```

CI config becomes minimal:

```yaml
# .gitlab-ci.yml
variables:
  TRAFIC_SSH_HOST: preprod.ovh.studiometa.fr
  TRAFIC_PROJECT_NAME: my-wordpress

include:
  - config/deploy/trafic/preprod.yml
```

#### Laravel Project

```toml
# .trafic.toml
[deploy]
script = "composer install --no-dev && php artisan migrate --force && php artisan config:cache"
after_script = "php artisan queue:restart"

[ddev]
php_version = "8.4"
database = { type = "postgresql", version = "16" }
additional_fqdns = ["staging.client-app.com"]
```

#### Static/Node Project (No DDEV Container)

```toml
# .trafic.toml
[deploy]
sync = ["dist/"]
no_start = true    # No DDEV container needed
script = ""        # Nothing to run in container
```

### Config Loading in the CLI

```typescript
import { readFile } from "node:fs/promises";
import { resolve } from "node:path";

interface ProjectConfig {
  deploy?: {
    script?: string;
    sync?: string[];
    before_script?: string;
    after_script?: string;
    no_start?: boolean;
  };
  ddev?: {
    php_version?: string;
    database?: { type: string; version: string };
    webserver_type?: string;
    additional_hostnames?: string[];
    additional_fqdns?: string[];
    hooks?: { post_start?: string[] };
  };
  backup?: {
    include?: string[];
  };
  auth?: {
    policy?: "basic" | "token" | "allow" | "deny";
  };
}

async function loadProjectConfig(dir = "."): Promise<ProjectConfig> {
  const configPath = resolve(dir, ".trafic.toml");
  try {
    const content = await readFile(configPath, "utf-8");
    // Parse TOML (need a TOML parser — smol-toml is 0-dep and tiny)
    const { parse } = await import("smol-toml");
    return parse(content) as ProjectConfig;
  } catch {
    return {}; // No config file — use defaults
  }
}
```

### TOML Parser Dependency

The CLI currently has zero runtime dependencies. Adding a TOML parser is the one exception we'd need.

| Package | Size | Dependencies | Notes |
|---------|------|-------------|-------|
| `smol-toml` | 12 KB | 0 | TOML 1.0 compliant, ESM, tiny |
| `@iarna/toml` | 23 KB | 0 | Popular, well-tested |
| Hand-rolled | ~200 lines | 0 | Only need basic key/value + sections |

**Recommendation**: `smol-toml` — single zero-dep package, 12 KB, full TOML 1.0 support. Acceptable trade-off for proper TOML parsing vs. hand-rolling a buggy subset parser.

### DDEV Config Generation

When the CLI detects `[ddev]` settings in `.trafic.toml`, it writes a `.ddev/config.local.yaml` on first deploy:

```yaml
# .ddev/config.local.yaml (auto-generated by trafic)
name: my-wordpress
override_config: true
php_version: "8.3"
database:
  type: mariadb
  version: "10.11"
webserver_type: nginx-fpm
```

This replaces the current minimal config write in `deploy.ts` and makes it project-aware.

### Agent-Side Config

The agent can also read `.trafic.toml` from deployed projects for:
- Per-project auth policy (override global auth rules)
- Backup includes
- Idle timeout override (some projects should never be stopped)

```toml
# .trafic.toml
[agent]
idle_timeout = "never"   # Never stop this project
```

---

## Summary & Priority

| Feature | Effort | Value | Priority | Phase |
|---------|--------|-------|----------|-------|
| Per-project config (`.trafic.toml`) | Medium | High | **P1** | Next — reduces CI boilerplate significantly |
| HTTPS (Let's Encrypt) | Low | High | **P1** | Next — just `trafic setup` config, DDEV does the work |
| Server hardening | Medium | High | **P1** | Next — security baseline for any public server |
| Backup solution | Medium | Medium | **P2** | After agent is ported — needs agent scheduled tasks |
| Fleet management | High | Medium | **P3** | Nice-to-have — useful for SM but not MVP for open-source |

### Suggested Implementation Order

1. **Per-project config** → Unblocks simpler CI configs and feeds into other features
2. **HTTPS setup** → Quick win, mostly config
3. **Server hardening** → Add to `trafic setup`, include `trafic audit`
4. **Backup** → After agent port, add `tasks/backup.ts` + `trafic backup` command
5. **Fleet** → After everything else is stable, add `trafic fleet` for multi-server ops
